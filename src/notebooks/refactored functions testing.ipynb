{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train_df = pd.read_csv(\"./base/Data/loan_data_2007_2014.csv\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# train_df.head()\n",
    "\n",
    "cols= [\"int_rate\", \"grade\", \"total_pymnt_inv\", \"term\", \"loan_status\", \"target\"]\n",
    "\n",
    "target_variable_bad_values = ['Charged Off','Late (31-120 days)','Default','Does not meet the credit policy. Status:Charged Off']\n",
    "\n",
    "train_df[\"target\"] = train_df[\"loan_status\"].apply(lambda x: 1 if x in target_variable_bad_values else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_rate</th>\n",
       "      <th>grade</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.65</td>\n",
       "      <td>B</td>\n",
       "      <td>5831.78</td>\n",
       "      <td>36 months</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.27</td>\n",
       "      <td>C</td>\n",
       "      <td>1008.71</td>\n",
       "      <td>60 months</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.96</td>\n",
       "      <td>C</td>\n",
       "      <td>3003.65</td>\n",
       "      <td>36 months</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.49</td>\n",
       "      <td>C</td>\n",
       "      <td>12226.30</td>\n",
       "      <td>36 months</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.69</td>\n",
       "      <td>B</td>\n",
       "      <td>3242.17</td>\n",
       "      <td>60 months</td>\n",
       "      <td>Current</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_rate grade  total_pymnt_inv        term  loan_status  target\n",
       "0     10.65     B          5831.78   36 months   Fully Paid       0\n",
       "1     15.27     C          1008.71   60 months  Charged Off       1\n",
       "2     15.96     C          3003.65   36 months   Fully Paid       0\n",
       "3     13.49     C         12226.30   36 months   Fully Paid       0\n",
       "4     12.69     B          3242.17   60 months      Current       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSquare test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np \n",
    "\n",
    "def getCramerv(c,n,contigency):\n",
    "        phi2 = c/n\n",
    "        r,k = contigency.shape\n",
    "        phi2corr = max(0.0, phi2 - (((k-1)*(r-1))/(n-1)))    \n",
    "        rcorr = r - ((r-1)**2)/(n-1)\n",
    "        kcorr = k - ((k-1)**2)/(n-1)\n",
    "        cramers_v =  np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "        return cramers_v\n",
    "\n",
    "def getChiSquare(dataset, cramersv_threshold=0.5,\n",
    "             write_output=False,output_path='', cardinality_threshold=15, dependent_variable=None):\n",
    "    \n",
    "    final_feature_set = dataset.columns.to_list()\n",
    "    chi_sqaure_data = {'Feature_Name':[],'Chi-Square':[],'P-Value':[],'dof':[],'Significant':[],'CramersV':[]}\n",
    "    alpha=0.05        \n",
    "    categoricals = dataset[final_feature_set].select_dtypes(include=['object','category']).columns\n",
    "    eliminated_features = []\n",
    "\n",
    "    for feature in categoricals:\n",
    "        if feature == dependent_variable:\n",
    "            continue\n",
    "        if(len(dataset[feature].unique()) > 15):   #Eliminating features with cardinality > 15\n",
    "            eliminated_features.append(feature)\n",
    "            continue\n",
    "\n",
    "        contigency= pd.crosstab(dataset[dependent_variable],dataset[feature])\n",
    "        c, p, dof, expected = chi2_contingency(contigency)\n",
    "        n =  sum(contigency.sum())\n",
    "        chi_sqaure_data[\"Feature_Name\"].append(feature)\n",
    "        chi_sqaure_data[\"Chi-Square\"].append(c)\n",
    "        chi_sqaure_data[\"P-Value\"].append(p)\n",
    "        chi_sqaure_data[\"dof\"].append(dof)            \n",
    "        chi_sqaure_data['CramersV'].append(getCramerv(c,n,contigency))\n",
    "    chi_sqaure_pd = pd.DataFrame(chi_sqaure_data,columns=['Feature_Name','Chi-Square','P-Value','dof','CramersV'])\n",
    "\n",
    "    #self.convert_df_to_html(chi_sqaure_pd,\"\",\"CramersV\")\n",
    "    cramersv_filter = chi_sqaure_pd[chi_sqaure_pd[\"CramersV\"]<=cramersv_threshold][\"Feature_Name\"].tolist()\n",
    "\n",
    "    #self.log('Eliminating following variables as CramersV below Threshold {}'.format(ExecutionStepInputs.CRAMERSV_THRESHOLD))  \n",
    "    #self.log(cramersv_filter)      \n",
    "    print('Eliminating following variables as CramersV below Threshold {}'.format(cramersv_threshold)) \n",
    "    print(cramersv_filter)\n",
    "\n",
    "    # self.log('Eliminating following variables as they have cardinality > 15 {}'.format(eliminated_features))\n",
    "    # self.log(eliminated_features)\n",
    "    print('Eliminating following variables as they have cardinality > 15 {}'.format(eliminated_features))\n",
    "    print(eliminated_features)\n",
    "\n",
    "    eliminated_features.extend(cramersv_filter)\n",
    "    for feature in eliminated_features:\n",
    "        final_feature_set.remove(feature)\n",
    "\n",
    "    if write_output:\n",
    "        chi_sqaure_pd.to_csv(output_path,index=False)\n",
    "\n",
    "    return chi_sqaure_pd, final_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminating following variables as CramersV below Threshold 0.5\n",
      "['grade', 'term']\n",
      "Eliminating following variables as they have cardinality > 15 []\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cols = [\"int_rate\", \"grade\", \"total_pymnt_inv\", \"term\", \"target\"]\n",
    "chi_pd, final_feature_set = getChiSquare(train_df[cols], cramersv_threshold=0.5, write_output=False, \n",
    "             output_path='', cardinality_threshold=15, dependent_variable=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Name</th>\n",
       "      <th>Chi-Square</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>dof</th>\n",
       "      <th>CramersV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grade</td>\n",
       "      <td>12806.998093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.16569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>term</td>\n",
       "      <td>1883.563272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature_Name    Chi-Square  P-Value  dof  CramersV\n",
       "0        grade  12806.998093      0.0    6   0.16569\n",
       "1         term   1883.563272      0.0    1   0.06354"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.crosstab(train_df[\"target\"],train_df[\"grade\"])\n",
    "c, p, dof, exp = chi2_contingency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cram_v(df, c):\n",
    "    n = sum(df.sum())\n",
    "    phi2 = c/n\n",
    "    r, k = df.shape\n",
    "    k_ = k - (((k-1)**2-1)/(n-1))\n",
    "    r_ = r - (((r-1)**2-1)/(n-1))\n",
    "    phi2_ = max(0, phi2 - ((k-1)*(r-1)/(n-1)))\n",
    "    cramers_v = np.sqrt(phi2_/min((k_-1), (r_-1)))\n",
    "    return cramers_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16568996027603455"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cram_v(df, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.crosstab(train_df[\"target\"],train_df[\"term\"])\n",
    "c,p,dof,ex = chi2_contingency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06354027177631306"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cram_v(df, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.index==0]/sum(df[df.index==0].sum())\n",
    "df2 = df[df.index==1]/sum(df[df.index==1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83/98764676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3455\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "df11 = df.T\n",
    "df11['0']/df11['0'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  filter by IV test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>71970</td>\n",
       "      <td>2897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>126170</td>\n",
       "      <td>10759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>110909</td>\n",
       "      <td>14384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>65040</td>\n",
       "      <td>11848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>28793</td>\n",
       "      <td>6964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>10037</td>\n",
       "      <td>3192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>2398</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target       0      1\n",
       "grade                \n",
       "A        71970   2897\n",
       "B       126170  10759\n",
       "C       110909  14384\n",
       "D        65040  11848\n",
       "E        28793   6964\n",
       "F        10037   3192\n",
       "G         2398    924"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df[cols].head()\n",
    "#temp_percentile_binning_train, feat_bins = pd.qcut(train_df[\"int_rate\"], 5, precision=5, retbins=True, duplicates='drop')\n",
    "contigency = pd.crosstab(train_df[\"grade\"], train_df['target'], dropna=True)\n",
    "contigency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_count = contigency[0].sum()\n",
    "bad_count = contigency[1].sum()\n",
    "contigency[\"Percentage_Good\"] = (contigency[0]/good_count)\n",
    "contigency[\"Percentage_Bad\"] = (contigency[1]/bad_count)\n",
    "contigency[\"woe\"] = np.log(contigency[\"Percentage_Good\"]/contigency[\"Percentage_Bad\"])\n",
    "contigency[\"woe\"] = contigency[\"woe\"].replace([np.nan, np.inf, -np.inf],0)\n",
    "contigency[\"IV\"] = (contigency[\"Percentage_Good\"] - contigency[\"Percentage_Bad\"]) * contigency[\"woe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29078213954085946"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contigency[\"IV\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_iv_score(feature_name,dataset, is_numeric=False):\n",
    "\n",
    "    if (is_numeric):\n",
    "        qlist = [np.round(i*0.1,1) for i in range(11)]\n",
    "        quart = dataset[feature_name].quantile(qlist)\n",
    "        temp_percentile_binning_train, feat_bins = pd.qcut(dataset[feature_name], 5 , precision=5, retbins=True, duplicates='drop' )\n",
    "        #self.percentile_bin_ranges[feature_name] = feat_bins            \n",
    "    if is_numeric: \n",
    "        #check shape of the below 2\n",
    "        contigency= pd.crosstab(temp_percentile_binning_train,dataset['target'],dropna=True)\n",
    "    else:\n",
    "        contigency= pd.crosstab(dataset[feature_name],dataset['target'],dropna=True)\n",
    "    chi_sqaure_data = {'Feature_Name':[],'WOE':[],'IV':[]}        \n",
    "#     if ('0' not in contigency.columns) and ('1' not in contigency.columns):\n",
    "#         print(feature_name)\n",
    "#         return 0\n",
    "    #print(contigency)\n",
    "    #contigency = contigency.T\n",
    "    good_count = contigency[0].sum()\n",
    "    bad_count = contigency[1].sum()\n",
    "    contigency[\"Percentage_Good\"] = (contigency[0]/good_count)\n",
    "    contigency[\"Percentage_Bad\"] = (contigency[1]/bad_count)\n",
    "    contigency[\"woe\"] = np.log(contigency[\"Percentage_Good\"]/contigency[\"Percentage_Bad\"])\n",
    "    contigency[\"woe\"] = contigency[\"woe\"].replace([np.nan, np.inf, -np.inf],0)\n",
    "    contigency[\"IV\"] = (contigency[\"Percentage_Good\"] - contigency[\"Percentage_Bad\"]) * contigency[\"woe\"]\n",
    "    #print(\"{} IV {}\".format(feature_name,contigency['IV'].sum()))\n",
    "    return contigency['IV'].sum()\n",
    "\n",
    "\n",
    "def filter_by_iv(dataset=None, iv_threshold=0.02,\n",
    "             write_output=False,output_path='', final_feature_set=None):\n",
    "    #self.log(\"Computing IV scores for all independent variables - \")\n",
    "    print(\"Computing IV scores for all independent variables - \")\n",
    "    feature_iv = {\"Feature\":[],\"IV\":[]}\n",
    "    if final_feature_set is None:\n",
    "        final_feature_set = dataset.columns.to_list()\n",
    "\n",
    "    numeric_features = dataset.select_dtypes(include=['number'])\n",
    "\n",
    "    for feature in final_feature_set:\n",
    "        feature_iv[\"Feature\"].append(feature)\n",
    "        iv = get_variable_iv_score(feature,dataset,feature in numeric_features)\n",
    "        feature_iv[\"IV\"].append(iv)   \n",
    "\n",
    "    #iv_pd[\"IV\"] = iv_pd[\"IV\"].replace([np.nan, np.inf, -np.inf],0)\n",
    "    iv_pd = pd.DataFrame(feature_iv)\n",
    "    ivs = dict(iv_pd.values)\n",
    "    iv_pd = iv_pd.sort_values('IV',ascending=False)\n",
    "\n",
    "\n",
    "    iv_filter = iv_pd[iv_pd[\"IV\"]<=iv_threshold][\"Feature\"].tolist()\n",
    "    #Filter out items based on iv < = 0.02 here & add to list.\n",
    "    #print(iv_filter)\n",
    "    #iv_pd.to_csv('Reports/IV_Report.csv',index=True)\n",
    "    #self.convert_df_to_html(iv_pd,self.pipeline_configuration['reports_directory'],'IV_Report',True)\n",
    "    #self.log('{}'.format(iv_pd))\n",
    "    #self.log(\"Removing following features are they are below the IV Threshold of {}\".format(ExecutionStepInputs.IV_THRESHOLD))        \n",
    "    print(\"Removing following features are they are below the IV Threshold of {}\".format(iv_threshold))\n",
    "    #self.log(iv_filter)\n",
    "    print(iv_filter)\n",
    "    #self.convert_df_to_html(iv_filter,self.pipeline_configuration[''],'IV_Report')        \n",
    "    for feature in iv_filter:\n",
    "        final_feature_set.remove(feature)\n",
    "\n",
    "    if write_output:\n",
    "        iv_pd.to_csv(output_path, index=False)\n",
    "\n",
    "    return iv_pd, final_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IV scores for all independent variables - \n",
      "Removing following features are they are below the IV Threshold of 0.02\n",
      "['target']\n"
     ]
    }
   ],
   "source": [
    "iv_pd,_ =  filter_by_iv(dataset=train_df[cols], iv_threshold=0.02,\n",
    "             write_output=False,output_path='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_pymnt_inv</td>\n",
       "      <td>0.549839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.309130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grade</td>\n",
       "      <td>0.290782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>term</td>\n",
       "      <td>0.038860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>target</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature        IV\n",
       "2  total_pymnt_inv  0.549839\n",
       "0         int_rate  0.309130\n",
       "1            grade  0.290782\n",
       "3             term  0.038860\n",
       "4           target  0.000000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iv_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correlation_matrix(dataset, correlation_threshold=0.7,\n",
    "                         write_output=False,output_path='', final_feature_set=None):\n",
    "    if final_feature_set is None:\n",
    "        final_feature_set = dataset.columns.to_list()\n",
    "    numeric_features = dataset[final_feature_set].select_dtypes(include=['number'])\n",
    "    correlation_matrix = numeric_features.corr()\n",
    "    correlation_matrix.index.name = 'numeric_features_list'\n",
    "\n",
    "    correlation_matrix.reset_index(inplace=True)\n",
    "\n",
    "    cols = correlation_matrix.columns\n",
    "    eliminated_features = set()\n",
    "\n",
    "    for feature1 in cols:\n",
    "        for feature2 in cols:\n",
    "            if (feature1 == feature2) or (feature1 == 'numeric_features_list') or (feature2 == 'numeric_features_list'):\n",
    "                continue\n",
    "            correlation = correlation_matrix.loc[correlation_matrix['numeric_features_list'] == feature1 , feature2 ].values[0]\n",
    "            if(correlation > correlation_threshold):\n",
    "                #Picking the feature with the higher IV of the 2 feature\n",
    "                if get_iv(feature1) > get_iv(feature2):\n",
    "                    eliminated_features.add(feature2)\n",
    "                else:\n",
    "                    eliminated_features.add(feature1)\n",
    "\n",
    "    #self.log('Correlation > Threshold {} hence eleminating {}'.format(ExecutionStepInputs.CORRELATION_THRESHOLD,str(eliminated_features)))\n",
    "    #print(eliminated_features)\n",
    "    print('Correlation > Threshold {} hence eleminating {}'.format(correlation_threshold,str(eliminated_features)))\n",
    "\n",
    "    #self.log(\"Generating Correlation Report\")\n",
    "    print(\"Generating Correlation Report\")\n",
    "    #self.convert_df_to_html(correlation_matrix,self.pipeline_configuration['reports_directory'],'Correlation_Report')\n",
    "    #self.log(correlation_matrix)\n",
    "    if write_output:\n",
    "        correlation_matrix.to_csv(output_path, index=False)\n",
    "\n",
    "    return correlation_matrix, eliminated_features, final_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation > Threshold 0.7 hence eleminating set()\n",
      "Generating Correlation Report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric_features_list</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129166</td>\n",
       "      <td>0.172361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>total_pymnt_inv</td>\n",
       "      <td>0.129166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.195169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>target</td>\n",
       "      <td>0.172361</td>\n",
       "      <td>-0.195169</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numeric_features_list  int_rate  total_pymnt_inv    target\n",
       "0              int_rate  1.000000         0.129166  0.172361\n",
       "1       total_pymnt_inv  0.129166         1.000000 -0.195169\n",
       "2                target  0.172361        -0.195169  1.000000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_matrix, _, __ = build_correlation_matrix(train_df[cols], correlation_threshold=0.7,\n",
    "                         write_output=False,output_path='', final_feature_set=None)\n",
    "\n",
    "cor_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_rate</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>int_rate</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129166</td>\n",
       "      <td>0.172361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <td>0.129166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.195169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.172361</td>\n",
       "      <td>-0.195169</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 int_rate  total_pymnt_inv    target\n",
       "int_rate         1.000000         0.129166  0.172361\n",
       "total_pymnt_inv  0.129166         1.000000 -0.195169\n",
       "target           0.172361        -0.195169  1.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[cols].select_dtypes(include=['number']).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute_vif test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def compute_vif(dataset=None, vif_threshold=10,\n",
    "             write_output=False, output_path='', final_feature_set=None):\n",
    "    if final_feature_set is None:\n",
    "        final_feature_set = dataset.columns.to_list()\n",
    "    feature_list = []\n",
    "    for feature in final_feature_set:\n",
    "        if(dataset.dtypes[feature] not in ['int64','float64']):\n",
    "            continue\n",
    "        else:\n",
    "            feature_list.append(feature)        \n",
    "\n",
    "    print(\"Computing VIFs to check for Multicolinearity between {} Features\".format(feature_list))\n",
    "    vif_ds = add_constant(dataset[feature_list])\n",
    "    vifs = pd.Series([variance_inflation_factor(vif_ds.values, i) for i in range(vif_ds.shape[1])], index=vif_ds.columns)        \n",
    "    #self.log(vifs)\n",
    "    vifs = vifs.sort_values(ascending=False)\n",
    "    #self.convert_df_to_html(vifs.to_frame(),self.pipeline_configuration['reports_directory'],'Multicolinearity_Report',hide_index=False)\n",
    "    eliminated_features = vifs[vifs >= vif_threshold].index.tolist()\n",
    "    #self.log(\"Removing features with VIFs greater than {}\".format(ExecutionStepInputs.VIF_THRESHOLD))\n",
    "    print(\"Removing features with VIFs greater than {}\".format(vif_threshold))\n",
    "    vifs = vifs[vifs <= vif_threshold]\n",
    "    #self.log(\"Eliminated following feature : {}\".format(eliminated_features))\n",
    "    print(\"Eliminated following feature : {}\".format(eliminated_features))\n",
    "    #vifs.to_csv(\"Reports/Multicolinearity_Report.csv\",index=True)\n",
    "    if write_output:\n",
    "        vifs.to_csv(output_path, index=False)\n",
    "\n",
    "    return vifs, final_feature_set, eliminated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicky/.local/lib/python3.8/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const              12.048226\n",
       "int_rate            1.060742\n",
       "total_pymnt_inv     1.069986\n",
       "target              1.084348\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vif_ds = add_constant(train_df[['int_rate', 'total_pymnt_inv', 'target']])\n",
    "vifs = pd.Series([variance_inflation_factor(vif_ds.values, i) for i in range(vif_ds.shape[1])], index=vif_ds.columns)\n",
    "vifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing VIFs to check for Multicolinearity between ['int_rate', 'total_pymnt_inv', 'target'] Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicky/.local/lib/python3.8/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing features with VIFs greater than 10\n",
      "Eliminated following feature : ['const']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target             1.084348\n",
       "total_pymnt_inv    1.069986\n",
       "int_rate           1.060742\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vifs,_,__ = compute_vif(dataset=train_df[cols], vif_threshold=10,\n",
    "             write_output=False, output_path='', final_feature_set=None)\n",
    "vifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for compute_csi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csi(training_set = None, oot_set=None, csi_threshold=0.25,\n",
    "             write_output=False,output_path='', final_feature_set=None):\n",
    "    if final_feature_set is None:\n",
    "        final_feature_set = training_set.columns.to_list()\n",
    "    csi_dict = { 'Feature' : [], 'CSI' : [] }\n",
    "    for feature in final_feature_set:\n",
    "        if(feature in [\"Unnamed: 0\",\"emp_title\",\"title\"]):\n",
    "            continue\n",
    "        #print(feature)\n",
    "        csi_dict['Feature'].append(feature)\n",
    "        csi_value = 0\n",
    "\n",
    "        if training_set.dtypes[feature] in ['int64','float64']:\n",
    "            csi_value = compute_csi_numeric(training_set[feature],oot_set[feature])\n",
    "        if training_set.dtypes[feature] in ['object']:\n",
    "            csi_value = compute_csi_categorical(training_set[feature],oot_set[feature])                                        \n",
    "        csi_dict['CSI'].append(round(csi_value,4))\n",
    "    csi_df = pd.DataFrame(csi_dict) \n",
    "    if write_output:\n",
    "        csi_df.to_csv(output_path,index=False)\n",
    "    return csi_df, final_feature_set\n",
    "\n",
    "def compute_csi_numeric(series1,series2):\n",
    "    #series1_endIndex = series1.count()\n",
    "\n",
    "#     min_value = round(min(series1.min(),series2.min()))         \n",
    "#     #print(f'min = {min_value}')\n",
    "#     max_value = round(max(series1.max(),series2.max()))\n",
    "#     #print(f'max = {max_value}')\n",
    "#     min_max_diff = round((max_value - min_value)/ExecutionStepInputs.PSI_BUCKET_COUNT)\n",
    "    #print(f'range = {min_max_diff}')\n",
    "    bin_range = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "    deciles = series1.quantile(bin_range)\n",
    "    #print(f'bin_range = {bin_range}')\n",
    "\n",
    "    df1 = pd.DataFrame(series1)\n",
    "    df2 = pd.DataFrame(series2)    \n",
    "\n",
    "    df1_bins = pd.Series(np.histogram(df1,bins=deciles)[0],name='expected')\n",
    "    df2_bins = pd.Series(np.histogram(df2,bins=deciles)[0],name='actual')\n",
    "\n",
    "    csi_df = pd.DataFrame([df1_bins,df2_bins])\n",
    "    csi_df = csi_df.T\n",
    "\n",
    "    #print(csi_df)        \n",
    "    csi_df['expected'] = csi_df['expected'] / (csi_df['expected'].sum())\n",
    "    csi_df['actual'] = csi_df['actual'] / (csi_df['actual'].sum())\n",
    "    csi_df['diff'] = csi_df['actual'] - csi_df['expected']\n",
    "    csi_df['psi'] = (csi_df['diff'])*np.log(csi_df['actual'] / csi_df['expected'])\n",
    "    csi_df['psi'] = csi_df['psi'].replace([np.nan, np.inf, -np.inf],0)\n",
    "\n",
    "    return csi_df['psi'].sum()\n",
    "\n",
    "def compute_csi_categorical(series1,series2):\n",
    "\n",
    "    df1_value_count = series1.value_counts()\n",
    "    df2_value_count = series2.value_counts()\n",
    "\n",
    "    df1_indexes = df1_value_count.index.tolist()\n",
    "    df2_indexes = df2_value_count.index.tolist()\n",
    "\n",
    "    categorical_values_list = list(set(df1_indexes+df2_indexes))        \n",
    "\n",
    "    val = {\"Values\":categorical_values_list}\n",
    "    df = pd.DataFrame(val)\n",
    "    df['Expected'] = 0 \n",
    "    df['Actual'] = 0 \n",
    "\n",
    "    for categorical_value in categorical_values_list:\n",
    "        if categorical_value in df1_indexes:\n",
    "            df.loc[(df.Values == categorical_value),'Expected'] = df1_value_count.loc[categorical_value]\n",
    "        if categorical_value in df2_indexes:\n",
    "            df.loc[(df.Values == categorical_value),'Actual'] = df2_value_count.loc[categorical_value]\n",
    "\n",
    "    df['Expected'] = df['Expected'] / df['Expected'].sum()\n",
    "    df['Actual'] = df['Actual'] / df['Actual'].sum()\n",
    "\n",
    "    df['Difference'] =  df['Actual'] - df['Expected']\n",
    "\n",
    "    df['CSI'] =  (df['Difference']) * np.log(df['Actual'] / df['Expected']) \n",
    "    df['CSI'] = df['CSI'].replace([np.nan, np.inf, -np.inf],0)\n",
    "\n",
    "    return df.CSI.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int_rate', 'grade', 'total_pymnt_inv', 'term', 'target']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicky/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (19,55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "oot_set = pd.read_csv(\"./base/Data/loan_data_2015.csv\")\n",
    "cols1 = ['int_rate', 'grade', 'total_pymnt_inv', 'term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>CSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.0868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grade</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_pymnt_inv</td>\n",
       "      <td>0.9781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>term</td>\n",
       "      <td>0.0130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature     CSI\n",
       "0         int_rate  0.0868\n",
       "1            grade  0.0067\n",
       "2  total_pymnt_inv  0.9781\n",
       "3             term  0.0130"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csi_df,  _ = compute_csi(training_set = train_df[cols1], oot_set=oot_set[cols1])\n",
    "csi_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
